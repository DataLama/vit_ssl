data:
  # batch_size 4096..
  batch_size: 64
  num_workers: 16
  preprocessing_num_workers: 8
  load_from_cache_file: True
  limit_train_samples:
  limit_val_samples:
  limit_test_samples:
  seed: 42

# ViT-B_16
model:
  image_size: 224
  patch_size: 16
  dim: 768 
  depth: 12 
  heads: 12 
  mlp_dim: 3072
  dim_head: 64
  dropout: 0.
  emb_dropout: 0.
  pool: cls
  
train:
  learning_rate: 0.003
  weight_decay: 0.3
  lr_scheduler_type: "cosine"
  num_warmup_steps: 10000
  max_train_steps: 
  gradient_accumulation_steps: 1
  num_train_epochs: 2

  