{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "900b8c25-86f9-4422-8ee7-15c4283dffbb",
   "metadata": {},
   "source": [
    "# An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale\n",
    "\n",
    "Reference:\n",
    "* https://github.com/lucidrains/vit-pytorch\n",
    "* https://github.com/huggingface/transformers/blob/v4.20.1/src/transformers/models/vit\n",
    "* https://github.com/google-research/vision_transformer\n",
    "\n",
    "## 1. Data\n",
    "\n",
    "### 1.1. Data Augmentation and Regularization\n",
    "* 공개된 코드에는 Aug는 `RandomResizedCrop`, `RandomHorizontalFlip`만 적용되어 있음. \n",
    "* 후속 논문에서 AugReg가 데이터 셋이 적은 경우에는 효과적이라고 함.\n",
    "* 하지만, 데이터가 커질 수록 Aug, Reg가 큰 효과를 발휘하지 못함.\n",
    "* 특히 Reg (e.g. dropout)의 경우는 오히려 성능을 하락시킴.\n",
    "* 반면, Aug는 epoch을 더 늘리면 그래도 성능 향상에 도움이 됨.\n",
    "\n",
    "## 2. Model\n",
    "\n",
    "### 2.1. Input Representation 구현\n",
    "\n",
    "<img src=\"assets/img_1.png\">\n",
    "\n",
    "### 2.2. Layer Norm의 위치.\n",
    "\n",
    "https://aclanthology.org/P19-1176.pdf\n",
    "\n",
    "<img src=\"assets/img_2.png\">\n",
    "\n",
    "### 2.3. einops로 구현하는 MHA\n",
    "\n",
    "[einops](https://github.com/arogozhnikov/einops)\n",
    "* einsum notation의 아이디어를 tensor manipulation으로 확장하여 사용할 수 있는 라이브러리\n",
    "* 다양한 딥러닝 framework에 interoprational 하다.\n",
    "\n",
    "<img src=\"assets/img_3.png\">\n",
    "\n",
    "## 3. Train\n",
    "\n",
    "### 3.1. Experiment Details\n",
    "\n",
    "<img src = \"assets/img_4.png\">\n",
    "\n",
    "* ImageNet Pretraining 하는 경우에는 strong한 regulariztion 학습하는데 도움이 되었음.\n",
    "* pretrain은 224 이미지에 대하여 진행함. fine-tuning은 384 이미지에 대하여 진행함.\n",
    "    * finetuning을 다른 resolution에 대하여 진행하는 것을 common practice라 함.\n",
    "* self-supervision MLM과 동일한 방식으로 해봤음.\n",
    "    * 1) predicting only the mean, 3bit color (i.e., 1 prediction of 512 colors) ??\n",
    "    * 2) predicting a 4 × 4 downsized version of the 16 × 16 patch with 3bit colors in parallel (i.e., 16 predictions of 512 colors) ??\n",
    "    * 3) 3) regression on the full patch using L2 (i.e., 256 regressions on the 3 RGB channels).\n",
    "    * 1) 옵션이 few-shot learning의 결과가 가장 잘 나왔고, 3)번은 결과가 잘 안나옴. 어찌되었든, image classification 기반의 pretrain보다는 성능이 안나옴.    \n",
    "\n",
    "### 3.2. TRANSFORMER SHAPE\n",
    "\n",
    "<img src = \"assets/img_5.png\">\n",
    "\n",
    "* Figure 8 shows 5-shot performance on ImageNet for different configurations. \n",
    "* based on ViT model with 8 layers, D = 1024, DMLP = 2048 and a patch size of 32.\n",
    "* We can see that scaling the depth results in the biggest improvements which are clearly visible up until 64 layer. (diminishing return after 16 layer.)\n",
    "* witdth는 효과가 미미함.\n",
    "* Decreasing the patch size and thus increasing the effective sequence length shows surprisingly robust improvements without introducing parameters.\n",
    "    * patch_size 줄이면 seq_len 늘어남 상대적인 computation이 늘어남.\n",
    "* 전반적으로 연산량이 늘어나면 비례하여 성능이 좋아짐. \n",
    "\n",
    "### 3.3. CLS vs GAP\n",
    "\n",
    "<img src = \"assets/img_6.png\">\n",
    "\n",
    "\n",
    "* CLS를 쓰거나, GAP를 쓰거나 성능에는 큰 차이가 없다. 다만 각각에 맞는 learning rate가 다르다.\n",
    "\n",
    "### 3.4. Positional Embedding\n",
    "\n",
    "<img src = \"assets/img_7.png\">\n",
    "\n",
    "\n",
    "* No pos, 2-D Pos, Rel Pos 다해봤는데, 1-D가 전반적인 성능이 가장 잘 나왔다.\n",
    "\n",
    "---\n",
    "\n",
    "# 함께 보면 좋을 것들.\n",
    "\n",
    "### [How to train your ViT? Data, Augmentation, and Regularization in Vision Transformers](https://arxiv.org/pdf/2106.10270.pdf)\n",
    "* 다양한 Data, AugReg들에 대하여 ViT를 어떻게 학습하면 좋을지 5만개의 ViT모델을 학습하면서 얻은 findings를 공유하는 페이퍼.\n",
    "\n",
    "### [Surrogate Gap Minimization Improves Sharpness-Aware Training](https://arxiv.org/pdf/2203.08065.pdf)\n",
    "> Empirically, GSAM consistently improves generalization (e.g., +3.2% over SAM and +5.4% over AdamW on ImageNet top-1 accuracy for ViT-B/32)\n",
    "* GSAM(Surrogate Gap Guided Sharpness-Aware Minimization) optimizer.\n",
    "* 기존 AdamW보다 ImageNet top-1 accuracy가 약 5.4% 상승함. (ViT-B/32)\n",
    "* ICLR2022 accepted paper\n",
    "\n",
    "### 다양한 vision transformers 관련 페이퍼\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
